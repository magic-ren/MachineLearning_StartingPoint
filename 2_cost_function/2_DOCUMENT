只有Rosenblatt感知器模型可以计算出w吗？现在已经很少用了，下面是现在常用的代价函数方法：

有两个点，现在有一条预测曲线，相对于这两个点的误差一个是1，一个是-1，那么1+（-1）=0，是不是最后误差就是0，也就是没有误差，显然不对。

可以取绝对值，但是这在数学处理和编码处理上都不是很方便。

对一批豆豆预测的结果和标准答案之间的平方误差越小，说明偏离事实越小。
每次w的不同，产生的误差不同。

所以，我们的问题就变成了寻找w，使得平方误差最小。

我们先取一个数据点来计算平方误差，最终因变量e，和自变量w是一个一元二次方程。（图1）
而整组数据的误差就是每个点的平方误差加起来然后再除以样本数，这也就是均方误差。通过数学换算最终也是一元二次方程。这也就是代价函数。（图2）
我们求出此代价函数的最低点的w（是开口向上的抛物线），就是我们最终需要的w

这种通过已知数据求参数的方式叫做回归分析。这次评估的标准是“均方误差”，并试图让它最小，这就叫做回归分析中的“最小二乘法”（图3）

代价函数：当参数w取不同值的时候，对环境中的问题数据预测时产生不同的误差e，
而利用这个代价函数的最低点的w值（开口向上的抛物线），然后把此w值放回到预测函数中，
这时候预测函数也就很好完成了对数据的拟合。

预测函数中，w作为参数，x是自变量输入，y是因变量输出，这是我们最终要得到的预测问题的函数。
而在研究代价函数的时候，x和y都是通过观测统计出来的已知数，成为了代价函数的已知参数部分。
而w成为了自变量，误差代价e是因变量。这是我们用来分析并改进预测函数的辅助函数。

那怎样得到最低点呢？可以用抛物线的顶点坐标公式来求w=-b/(2a) 。
这种求解参数的方式叫做正规方程。当数据量小的时候很合适，可是机器学习都是海量数据，这种
一步到位的方式就意味着巨大的计算量和存储量。


绝对差：一个值和目标值的误差的绝对值，
平方误差：一个值和目标值的误差的平方
均方误差：一组数，每个数的平方误差的和，然后求其平均值
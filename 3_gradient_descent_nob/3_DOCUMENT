除了正规方程一步到位外，还可以慢慢挪到最低点。

怎么挪？

思路
最低点的斜率为0，左边的斜率为负，右边的斜率为正。只要将斜率不断的挪向0就行。（见图1）

斜率怎么求？
1）当曲线上的两个点无限接近时就成了直线，就可以用纵坐标差值除以横坐标差值求了。（见图2）
在曲线中，某处的斜率一般称之为导数。用纵坐标差值除以横坐标差值，并取极限，这就是导数的定义。这种求导的方法也叫定义法。（见图3）
2）既然定义法可以得到一个函数的导数，而数学里常用的函数也就那么多，那么把这些函数的导数全部求出来形成一张固定公式的表格，然后用的时候去查询。（见图4）
但是它们有无穷无尽的组合。其实组合也就三种方式：加法、减法、组合（见图5、图6）

现在知道每个点的斜率了，那么具体怎么挪呢？
1）固定步长，比如每次挪0.01。这样开始的时候调整的速度很慢，而且在底部附近还会反复震荡。因为步长是固定的。
2）想要的效果是，离最低点远的时候调整快一点，接近最低点时调整幅度变小。
     斜率正好是在离最低点远的时候绝对值大，接近最低点时绝对值小。而斜率在最低点左右的符号又正好不同。
     所以，可以用斜率做这件事，w=w-斜率。
     但是正如rosenblatt感知器里那样振动幅度过大，所以这里也要加一个学习率。w=w-α*斜率。
     这种根据曲线不同处“斜率”去调整w的方式，就是“梯度下降”

梯度下降的几种方式：
1）在所有样本合成的代价函数上做梯度下降，叫“批量梯度下降”；（这样和正规方程没啥区别了，都要并行大量计算）
2）每个样本在它自己的代价函数上做梯度下降，将每个样本依次这样做。因为其收敛的过程是一个随机震荡的轨迹，所以叫“随机梯度下降”；（单个样本的代价函数见图7）
3）将1和2调和，"mini batch"，叫“mini批量梯度下降”，每次选择样本中的一小批比如100、200个进行批量梯度下降。


代码里用的是随机梯度下降算法(stochastic gradient descent,SGD)
还可以写固定步长下降、批量梯度下降、mini批量梯度下降
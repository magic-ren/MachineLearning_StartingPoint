y=wx;
y=wx+b;
项目3中的预测曲线只能通过原点，所以它的自由度是受到限制的，这个项目增加截距b，让这个预测曲线在平面内真正自由起来。

可以把代价函数整理成以w为自变量的方程，项目3中的情况就是这里b取0的特殊情况。（见图1）
也可以把代价函数整理成以b为自变量的方程。（见图2）

把代价函数看做是e关于w的一元二次函数曲线在b取不同值的时候形成的以外（见图3），也可以认为是e关于b的一元二次函数曲线在每次w取不同的值的时候形成的。（见图4）

把两个方向上的调整运动合成一个合成调整运动，这样就完成了一次调整，反复进行这个过程，也就逐渐向这个曲面的最低点挪动。曲面的最低点就是我们要求的参数。（见图5）

对于有两个自变量的代价函数，先偏向w求导数，再偏向b求导数。
为了区分只有一个自变量的情况，我们把在某一个变量上的导数也称之为“偏导数”。
如果我们把对w和对b的偏导数看作向量，把这两个向量合在一起，形成一个新的合向量，沿着这个合向量进行下降，是这个曲面在该点下降最快的方式，这个合向量在数学里称之为“梯度”。（见图6）
这就是为什么梯度是比斜率更广泛的一个概念，他是把各个方向上的偏导数当做向量合起来形成的一个总向量，代表着这个点下降最快的方向。

把我们通过统计观测而来的数据送入预测函数进行预测的过程，就称之为前向传播，因为计算从前往后。
数据通过预测函数完成一次前向传播就会得到一个预测值，预测值和统计观测而来的真实值之间存在着误差，我们选择均方误差作为评估的手段，
你会发现这个误差和预测函数中的参数又会形成一种函数关系，我们把这个函数称之为“代价函数”，因为采用方差去评估预测误差，
所以也称之为“方差代价函数”，描述了预测函数的参数取不同值的时候，预测的不同的误差代价。
而用这个代价函数去修正预测函数参数的过程，也称之为“反向传播”，因为计算从后往前。
而这个反向传播的参数修正方法，我们使用“梯度下降”算法。
而在调整过程中用来调和下降幅度的α，称之为“学习率”。它的选择影响调整的速度，太大了容易反复横跳，过大的时候甚至不会收敛
而发散，太小了调整速度又太慢了，它是设计者根据经验选择出来的。
不断的经历前向传播和反向传播最终到达代价函数最低点的过程，我们称之为“训练”或者“学习”。（见图7）


这就是所谓的机器学习中的神经网络。
把一个神经元称之为网络不太恰当，因为没有哪一个网络只有一个节点。但我们以后不断的添加神经元，并将他们连接起来共同工作的时候，
也就能称之为“神经网络”。而我们所说的“前向传播”和“反向传播”，其实也是在多层神经网络出现后才引入的概念。对于单个神经元，
如此称呼似乎有点别扭，但这些概念在单个神经元上已经具备雏形，面对网络不过是不断的重复而已。（见图8、图9）
这就是目前人工智能机器学习领域独领风骚的“联结主义”。